{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AugusGuarna/Feedforward_NN/blob/main/NN_multiclass_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg8w63XDJlZ2"
      },
      "source": [
        "# Neural Network for Multiclass Prediction\n",
        "\n",
        "This notebook implements a **feedforward neural network** for multiclass classification using the **Iris dataset** and **PyTorch**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Architecture\n",
        "\n",
        "We define the network layout as follows:\n",
        "\n",
        "| Layer | Description |\n",
        "|-------|-------------|\n",
        "| **Input** | Vector in $\\mathbb{R}^{4}$ (4 features per sample) |\n",
        "| **Hidden** | 3 layers with **6**, **8**, and **10** neurons respectively |\n",
        "| **Output** | Vector in $\\mathbb{R}^{3}$ (3 classes) |\n",
        "| **Activations** | **ReLU** in hidden layers, **Softmax** at the output (via cross-entropy) |\n",
        "\n",
        "The input size matches the 4 Iris attributes (e.g. sepal length, width; petal length, width), and the output size matches the 3 species: `setosa`, `versicolor`, and `virginica`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "uflcON-HxJ1O"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "7wd_JnzutbvQ"
      },
      "outputs": [],
      "source": [
        "# --- Load Iris and build DataLoaders ---\n",
        "dataset = load_iris()\n",
        "\n",
        "X = dataset[\"data\"]\n",
        "y = dataset[\"target\"]\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "full_dataset = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y))\n",
        "\n",
        "# 90% train / 10% test (small dataset)\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "batch_size = 5\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "XbRqFdOjJF2s"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "    \"\"\"Feedforward NN with configurable layers; ReLU in hidden layers, linear output (softmax in loss).\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, number_layers, size_layers):\n",
        "        super().__init__()\n",
        "        self.weights = nn.ParameterList()\n",
        "        self.bias = nn.ParameterList()\n",
        "        self.number_layers = number_layers\n",
        "        self.size_layers = size_layers\n",
        "        for i in range(number_layers + 1):\n",
        "            if i == 0:\n",
        "                self.weights.append(nn.Parameter(torch.randn(size=(input_size, size_layers[i]),\n",
        "                                                             dtype=torch.float, requires_grad=True)))\n",
        "                self.bias.append(nn.Parameter(torch.randn(size_layers[i]), requires_grad=True))\n",
        "            elif i > 0 and i < number_layers:\n",
        "                self.weights.append(nn.Parameter(torch.randn(size=(size_layers[i - 1], size_layers[i]),\n",
        "                                                             dtype=torch.float, requires_grad=True)))\n",
        "                self.bias.append(nn.Parameter(torch.randn(size_layers[i]), requires_grad=True))\n",
        "            else:\n",
        "                self.weights.append(nn.Parameter(torch.randn(size=(size_layers[i - 1], output_size),\n",
        "                                                             dtype=torch.float, requires_grad=True)))\n",
        "                self.bias.append(nn.Parameter(torch.randn(output_size), requires_grad=True))\n",
        "\n",
        "    def forward(self, data):\n",
        "        Z = data\n",
        "        for i in range(len(self.weights)):\n",
        "            Z = torch.matmul(Z, self.weights[i]) + self.bias[i]\n",
        "            if i < len(self.weights) - 1:\n",
        "                Z = F.relu(Z)\n",
        "        return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "0XmrOang5BXE"
      },
      "outputs": [],
      "source": [
        "# --- Instantiate model: 4 inputs, 3 outputs, 3 hidden layers (6, 8, 10) ---\n",
        "torch.manual_seed(42)\n",
        "model0 = FeedForwardNeuralNetwork(4, 3, 3, [6, 8, 10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXv796Ia5ePE",
        "outputId": "559decb8-8045-4d8e-8db0-5c2bbf1c2074"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weights.0',\n",
              "              tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345],\n",
              "                      [-0.0431, -1.6047,  0.3559, -0.6866, -0.4934,  0.2415],\n",
              "                      [-1.1109,  0.0915, -2.3169, -0.2168, -0.3097, -0.3957],\n",
              "                      [ 0.8034, -0.6216, -0.5920, -0.0631, -0.8286,  0.3309]])),\n",
              "             ('weights.1',\n",
              "              tensor([[-0.4880,  1.1914, -0.8140, -0.7360, -0.8371, -0.9224,  1.8113,  0.1606],\n",
              "                      [-0.0978,  1.8446, -1.1845,  1.3835, -1.2024,  0.7078, -1.0759,  0.5357],\n",
              "                      [ 0.3466, -0.1973, -1.0546,  1.2780,  0.1453,  0.2311,  0.0087, -0.1423],\n",
              "                      [ 0.5750, -0.6417, -2.2064, -0.7508,  2.8140,  0.3598, -0.0898,  0.4584],\n",
              "                      [ 0.5362,  0.5246,  1.1412,  0.0516,  0.7281, -0.7106, -0.6021,  0.9604],\n",
              "                      [-1.7223, -0.8278,  1.3347,  0.4835, -0.1976,  1.2683,  1.2243,  0.0981]])),\n",
              "             ('weights.2',\n",
              "              tensor([[ 0.7262,  0.0912, -0.3891,  0.5279,  1.0311, -0.7048,  1.0131, -0.3308,\n",
              "                        1.0950,  0.3399],\n",
              "                      [ 0.7200,  0.4114, -0.5733,  0.5069, -0.4752, -0.4920, -0.1360,  1.6354,\n",
              "                        0.6547,  0.5760],\n",
              "                      [-0.3609, -0.0606,  0.0733,  0.8187, -0.3753,  1.0331, -0.6867,  0.6368,\n",
              "                        0.2176, -0.0467],\n",
              "                      [-1.4335, -0.5665,  0.2695, -0.2104, -0.7328,  0.1043,  1.0414, -0.3997,\n",
              "                       -2.2933,  0.4976],\n",
              "                      [-2.4801, -0.4175, -1.1955,  0.8123, -0.3063, -0.3302, -0.9808,  0.1947,\n",
              "                        0.2868, -0.7308],\n",
              "                      [ 0.1748, -1.0939,  0.9633, -0.3095,  0.5712,  1.1179, -1.5469,  0.7567,\n",
              "                        0.7755,  2.0265],\n",
              "                      [ 0.9812, -0.6401, -0.4908,  0.2080, -0.9319, -1.5910, -1.1360, -0.5226,\n",
              "                        0.7165,  1.5335],\n",
              "                      [-1.4510, -0.7861,  1.0229, -0.5558,  0.7043,  0.7099, -1.5326, -0.7251,\n",
              "                        0.4664,  0.6667]])),\n",
              "             ('weights.3',\n",
              "              tensor([[ 1.3032,  0.4879,  1.1340],\n",
              "                      [-0.3556,  0.3618,  1.9993],\n",
              "                      [ 0.6630,  0.7047,  0.0213],\n",
              "                      [-0.8293, -1.0809, -0.7839],\n",
              "                      [ 0.5071,  0.0821, -0.3532],\n",
              "                      [ 1.4639,  0.1729,  1.0514],\n",
              "                      [ 0.0075, -0.0774,  0.6427],\n",
              "                      [ 0.5742,  0.5058,  0.2225],\n",
              "                      [-0.9143,  1.4840, -0.9109],\n",
              "                      [-0.5291, -0.8051,  0.5158]])),\n",
              "             ('bias.0',\n",
              "              tensor([ 1.3525,  0.6863, -0.3278,  0.7950,  0.2815,  0.0562])),\n",
              "             ('bias.1',\n",
              "              tensor([-0.2858, -1.0935,  1.1351,  0.7592, -3.5945,  0.0192,  0.1052,  0.9603])),\n",
              "             ('bias.2',\n",
              "              tensor([ 2.3839,  0.9157, -0.6430,  0.7113,  0.4000, -1.2039, -0.4198, -1.1929,\n",
              "                      -0.9351,  0.2138])),\n",
              "             ('bias.3', tensor([ 0.8228, -0.8046, -0.3734]))])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sanity check: inspect initialized parameters\n",
        "model0.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAV_vFT07kAz"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Training and Testing\n",
        "\n",
        "We train the network and evaluate it on the held-out test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwvr9-P7sIP"
      },
      "source": [
        "**Setup:**\n",
        "\n",
        "- **Loss:** Cross-entropy (includes softmax).\n",
        "- **Optimizer:** `torch.optim.SGD` with learning rate `0.01`.\n",
        "- **Epochs:** 200.\n",
        "- **Metrics:** Train/test loss and accuracy (logged every 10 epochs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "agN-VxWh7r5u"
      },
      "outputs": [],
      "source": [
        "# Loss (cross-entropy applies softmax internally) and optimizer\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model0.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqcAwoHS7deo",
        "outputId": "06209e4a-3dd6-4c62-f6f9-c214a26ffe1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Train Loss: 0.06046| Test Loss: 0.07228\n",
            "Train acc: 0.9703703703703703 | Test acc: 0.9333333333333333\n",
            "Epoch: 10 | Train Loss: 0.06365| Test Loss: 0.01664\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 20 | Train Loss: 0.06075| Test Loss: 0.03326\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 30 | Train Loss: 0.05966| Test Loss: 0.02613\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 40 | Train Loss: 0.05768| Test Loss: 0.01355\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 50 | Train Loss: 0.08195| Test Loss: 0.03703\n",
            "Train acc: 0.9555555555555556 | Test acc: 1.0\n",
            "Epoch: 60 | Train Loss: 0.05170| Test Loss: 0.01182\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 70 | Train Loss: 0.06519| Test Loss: 0.03497\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 80 | Train Loss: 0.06684| Test Loss: 0.02449\n",
            "Train acc: 0.9629629629629629 | Test acc: 1.0\n",
            "Epoch: 90 | Train Loss: 0.05581| Test Loss: 0.02706\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 100 | Train Loss: 0.05581| Test Loss: 0.02892\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 110 | Train Loss: 0.08453| Test Loss: 0.01415\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 120 | Train Loss: 0.05557| Test Loss: 0.02259\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 130 | Train Loss: 0.08466| Test Loss: 0.01896\n",
            "Train acc: 0.9629629629629629 | Test acc: 1.0\n",
            "Epoch: 140 | Train Loss: 0.06213| Test Loss: 0.05309\n",
            "Train acc: 0.9703703703703703 | Test acc: 0.9333333333333333\n",
            "Epoch: 150 | Train Loss: 0.05186| Test Loss: 0.02056\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 160 | Train Loss: 0.07260| Test Loss: 0.01769\n",
            "Train acc: 0.9629629629629629 | Test acc: 1.0\n",
            "Epoch: 170 | Train Loss: 0.05541| Test Loss: 0.02414\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 180 | Train Loss: 0.05414| Test Loss: 0.02713\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 190 | Train Loss: 0.07588| Test Loss: 0.02666\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n"
          ]
        }
      ],
      "source": [
        "# --- Training loop (200 epochs) ---\n",
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 200\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model0.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Loop through training batches\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # 1. Forward pass\n",
        "        y_pred = model0(X_batch)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_f(y_pred, y_batch)\n",
        "\n",
        "        # 3. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate batch loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate the softmax over the predictions in order to compute accuracy\n",
        "        predicted = y_pred.argmax(dim=1)\n",
        "        train_acc += (predicted == y_batch).sum().item()\n",
        "        train_total += y_batch.size(0)\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_acc /=  train_total\n",
        "\n",
        "    # Evaluation on test set\n",
        "    model0.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    test_total = 0\n",
        "    with torch.inference_mode():\n",
        "        # Loop through test batches\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            # 1. Forward pass on test data\n",
        "            test_pred = model0(X_batch)\n",
        "\n",
        "            # 2. Calculate test loss\n",
        "            batch_test_loss = loss_f(test_pred, y_batch)\n",
        "\n",
        "            # Accumulate batch test loss\n",
        "            test_loss += batch_test_loss.item()\n",
        "\n",
        "            # Calculate the softmax over the predictions in order to compute accuracy\n",
        "            predicted = test_pred.argmax(dim=1)\n",
        "            test_acc += (predicted == y_batch).sum().item()\n",
        "            test_total += y_batch.size(0)\n",
        "\n",
        "        # Calculate average test loss for the epoch\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc /= test_total\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        epoch_count.append(epoch)\n",
        "        train_loss_values.append(train_loss)\n",
        "        test_loss_values.append(test_loss)\n",
        "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f}| Test Loss: {test_loss:.5f}\")\n",
        "        print(f\"Train acc: {train_acc} | Test acc: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwVNqVEuhmCZ"
      },
      "source": [
        "**Summary:** The network achieves high accuracy on this architecture and classification task. The very high test accuracy may be partly due to the small size of the dataset; with more data, a proper validation strategy would give a clearer picture of generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4qT_-qchl11"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMbcYyIakVSKiqPG6OZ57fu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
